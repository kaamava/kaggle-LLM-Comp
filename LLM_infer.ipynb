{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I previously shared a [notebook](url) (see the discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/441128)) that found a cluster of relevant Wikipedia STEM articles, resulting in around 270K STEM articles for which the resulting dataset is released [here.](https://www.kaggle.com/datasets/mbanaei/stem-wiki-cohere-no-emb)\n",
    "\n",
    "However, due to issues with WikiExtractor, there're cases in which some numbers or even paragraphs are missing from the final Wiki parsing. Therefore,  for the same set of  articles, I used Wiki API to gather the articles' contexts (see discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/442483)), for which the resulting dataset is released [here](https://www.kaggle.com/datasets/mbanaei/all-paraphs-parsed-expanded).\n",
    "\n",
    "In order to show that the found articles cover not only the train dataset articles but also a majority of LB gold articles, I release this notebook that uses a simple retrieval model (without any prior indexing) together with a model that is trained only on the RACE dataset. (not fine-tuned on any competition-similar dataset).\n",
    "\n",
    "The main design choices for the notebook are:\n",
    "- Using a simple TF-IDF to retrieve contexts from both datasets for every given question.\n",
    "- Although the majority of high-performing public models use DeBERTa-V3 to do the inference in their pipeline, I used a LongFormer Large model, which enables us to have a much longer prefix context given limited GPU memory. More specifically, as opposed to many public notebooks, there's no splitting to sentence level, and the whole paragraph is retrieved and passed to the classifier as a context (the main reason that we don't get OOM and also have relatively fast inference is that in LongFormer full attention is not computed as opposed to standard models like BERT).\n",
    "- I use a fall-back model (based on a public notebook that uses an openbook approach and performs 81.5 on LB) that is used for prediction when there's low confidence in the main model's output for the top choice.\n",
    "\n",
    "P.S: Although the model's performance is relatively good compared to other public notebooks, many design choices can be revised to improve both inference time and performance. (e.g., currently, context retrieval seems to be the inference bottleneck as no prior indexing is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:31:25.966035Z",
     "iopub.status.busy": "2023-10-03T09:31:25.965686Z",
     "iopub.status.idle": "2023-10-03T09:32:03.344406Z",
     "shell.execute_reply": "2023-10-03T09:32:03.343075Z",
     "shell.execute_reply.started": "2023-10-03T09:31:25.966005Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n",
    "!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n",
    "!cp /kaggle/input/backup-806/util_openbook.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:32:03.347365Z",
     "iopub.status.busy": "2023-10-03T09:32:03.346736Z",
     "iopub.status.idle": "2023-10-03T09:34:06.971438Z",
     "shell.execute_reply": "2023-10-03T09:34:06.970379Z",
     "shell.execute_reply.started": "2023-10-03T09:32:03.347326Z"
    }
   },
   "outputs": [],
   "source": [
    "# installing offline dependencies\n",
    "!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:34:06.974054Z",
     "iopub.status.busy": "2023-10-03T09:34:06.973635Z",
     "iopub.status.idle": "2023-10-03T09:34:07.102904Z",
     "shell.execute_reply": "2023-10-03T09:34:07.101726Z",
     "shell.execute_reply.started": "2023-10-03T09:34:06.974017Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-03T09:35:33.790960Z",
     "iopub.status.busy": "2023-10-03T09:35:33.790369Z",
     "iopub.status.idle": "2023-10-03T09:35:45.273901Z",
     "shell.execute_reply": "2023-10-03T09:35:45.272952Z",
     "shell.execute_reply.started": "2023-10-03T09:35:33.790928Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForMultipleChoice\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:35:45.275606Z",
     "iopub.status.busy": "2023-10-03T09:35:45.275205Z",
     "iopub.status.idle": "2023-10-03T09:36:10.919621Z",
     "shell.execute_reply": "2023-10-03T09:36:10.918323Z",
     "shell.execute_reply.started": "2023-10-03T09:35:45.275564Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n",
    "!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:36:10.922559Z",
     "iopub.status.busy": "2023-10-03T09:36:10.922079Z",
     "iopub.status.idle": "2023-10-03T09:36:10.944203Z",
     "shell.execute_reply": "2023-10-03T09:36:10.943162Z",
     "shell.execute_reply.started": "2023-10-03T09:36:10.922465Z"
    }
   },
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    df_chunk_size=600\n",
    "    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "    modified_texts = paraphs_parsed_dataset.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    df_chunk_size=800\n",
    "    \n",
    "    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "    modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    \n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 sublinear_tf=True)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid,\n",
    "                                 sublinear_tf=True)\n",
    "    vectorizer.fit(modified_texts[:500000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100000\n",
    "    top_per_chunk = 10\n",
    "    top_per_query = 10\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:36:10.949204Z",
     "iopub.status.busy": "2023-10-03T09:36:10.948681Z",
     "iopub.status.idle": "2023-10-03T09:36:10.962098Z",
     "shell.execute_reply": "2023-10-03T09:36:10.961276Z",
     "shell.execute_reply.started": "2023-10-03T09:36:10.949167Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:36:10.964267Z",
     "iopub.status.busy": "2023-10-03T09:36:10.963653Z",
     "iopub.status.idle": "2023-10-03T09:36:11.005168Z",
     "shell.execute_reply": "2023-10-03T09:36:11.004399Z",
     "shell.execute_reply.started": "2023-10-03T09:36:10.964238Z"
    }
   },
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:36:11.006762Z",
     "iopub.status.busy": "2023-10-03T09:36:11.006282Z",
     "iopub.status.idle": "2023-10-03T09:43:55.886071Z",
     "shell.execute_reply": "2023-10-03T09:43:55.884939Z",
     "shell.execute_reply.started": "2023-10-03T09:36:11.006734Z"
    }
   },
   "outputs": [],
   "source": [
    "retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:43:55.888097Z",
     "iopub.status.busy": "2023-10-03T09:43:55.887775Z",
     "iopub.status.idle": "2023-10-03T09:53:09.115568Z",
     "shell.execute_reply": "2023-10-03T09:53:09.114431Z",
     "shell.execute_reply.started": "2023-10-03T09:43:55.888066Z"
    }
   },
   "outputs": [],
   "source": [
    "retrieved_articles = get_relevant_documents(df_valid)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:53:09.122311Z",
     "iopub.status.busy": "2023-10-03T09:53:09.119787Z",
     "iopub.status.idle": "2023-10-03T09:53:09.253902Z",
     "shell.execute_reply": "2023-10-03T09:53:09.251189Z",
     "shell.execute_reply.started": "2023-10-03T09:53:09.122274Z"
    }
   },
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for index in tqdm(range(df_valid.shape[0])):\n",
    "    columns = df_valid.iloc[index].values\n",
    "    question = columns[1]\n",
    "    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n",
    "    context1 = f\"{retrieved_articles[index][-1][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-4][2]}\"\n",
    "    contexts.append(context1)\n",
    "df_valid['context'] = contexts\n",
    "df_valid[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"test_context1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:53:09.275540Z",
     "iopub.status.busy": "2023-10-03T09:53:09.274757Z",
     "iopub.status.idle": "2023-10-03T09:53:09.386507Z",
     "shell.execute_reply": "2023-10-03T09:53:09.385641Z",
     "shell.execute_reply.started": "2023-10-03T09:53:09.275501Z"
    }
   },
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for index in tqdm(range(df_valid.shape[0])):\n",
    "    columns = df_valid.iloc[index].values\n",
    "    question = columns[1]\n",
    "    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n",
    "    context2 = f\"{retrieved_articles_parsed[index][-1][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-3][2]}\"\n",
    "    contexts.append(context2)\n",
    "df_valid['context'] = contexts\n",
    "df_valid[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"test_context2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:53:09.392342Z",
     "iopub.status.busy": "2023-10-03T09:53:09.390281Z",
     "iopub.status.idle": "2023-10-03T09:53:09.420147Z",
     "shell.execute_reply": "2023-10-03T09:53:09.419274Z",
     "shell.execute_reply.started": "2023-10-03T09:53:09.392307Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_context1.csv\")\n",
    "test_df.index = list(range(len(test_df)))\n",
    "test_df['id'] = list(range(len(test_df)))\n",
    "test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:3400]) + \" #### \" +  test_df[\"prompt\"]\n",
    "test_df['answer'] = 'A'\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "def preprocess(example):\n",
    "    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "    # so we'll copy our question 5 times before tokenizing\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:55:05.094336Z",
     "iopub.status.busy": "2023-10-03T09:55:05.093948Z",
     "iopub.status.idle": "2023-10-03T09:55:06.711161Z",
     "shell.execute_reply": "2023-10-03T09:55:06.710271Z",
     "shell.execute_reply.started": "2023-10-03T09:55:05.094305Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:55:19.155048Z",
     "iopub.status.busy": "2023-10-03T09:55:19.154704Z",
     "iopub.status.idle": "2023-10-03T09:55:19.164088Z",
     "shell.execute_reply": "2023-10-03T09:55:19.163130Z",
     "shell.execute_reply.started": "2023-10-03T09:55:19.155021Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:55:22.142791Z",
     "iopub.status.busy": "2023-10-03T09:55:22.142456Z",
     "iopub.status.idle": "2023-10-03T09:55:47.895241Z",
     "shell.execute_reply": "2023-10-03T09:55:47.894291Z",
     "shell.execute_reply.started": "2023-10-03T09:55:22.142764Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = \"modelfolder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:55:51.110622Z",
     "iopub.status.busy": "2023-10-03T09:55:51.110242Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions1 = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions1.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions1 = torch.cat(test_predictions1)\n",
    "test_predictions1 = test_predictions1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T09:53:10.410515Z",
     "iopub.status.idle": "2023-10-03T09:53:10.413340Z",
     "shell.execute_reply": "2023-10-03T09:53:10.413113Z",
     "shell.execute_reply.started": "2023-10-03T09:53:10.413090Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_context2.csv\")\n",
    "test_df.index = list(range(len(test_df)))\n",
    "test_df['id'] = list(range(len(test_df)))\n",
    "test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:3400]) + \" #### \" +  test_df[\"prompt\"]\n",
    "test_df['answer'] = 'A'\n",
    "tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "test_predictions2 = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions2.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions2 = torch.cat(test_predictions2)\n",
    "test_predictions2 = test_predictions2.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T09:53:10.422270Z",
     "iopub.status.idle": "2023-10-03T09:53:10.423021Z",
     "shell.execute_reply": "2023-10-03T09:53:10.422810Z",
     "shell.execute_reply.started": "2023-10-03T09:53:10.422789Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = test_predictions1+test_predictions2\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_string = test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "submission = test_df[['id', 'prediction']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
